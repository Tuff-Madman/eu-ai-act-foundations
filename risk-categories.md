---
layout: default
title: Risk Categories | EU AI Act
description: Detailed explanation of the EU AI Act's risk-based approach, including prohibited, high-risk, limited risk, and minimal risk AI systems.
---

# EU AI Act Risk Categories

<div class="card">
  <h3>Risk-Based Regulation</h3>
  <p>The EU AI Act adopts a risk-based approach to regulating artificial intelligence, with different requirements depending on the risk level of your AI system. This page explains the four main risk categories and helps you determine where your AI system fits.</p>
</div>

## Risk Classification Overview

<div class="card">
  <h3>Four-Tier Risk Structure</h3>
  <p>The EU AI Act categorizes AI systems into four main risk levels, with regulatory requirements proportionate to the level of risk:</p>
  
  <div class="risk-pyramid">
    <div class="risk-level risk-prohibited">
      <h4>ğŸš« Unacceptable Risk</h4>
      <p>Prohibited AI systems</p>
    </div>
    <div class="risk-level risk-high">
      <h4>âš ï¸ High Risk</h4>
      <p>Subject to strict requirements</p>
    </div>
    <div class="risk-level risk-limited">
      <h4>âš¡ Limited Risk</h4>
      <p>Subject to transparency obligations</p>
    </div>
    <div class="risk-level risk-minimal">
      <h4>âœ… Minimal Risk</h4>
      <p>Minimal or no obligations</p>
    </div>
  </div>
</div>

## ğŸš« Prohibited AI Systems (Unacceptable Risk)

<div class="card">
  <h3>Banned Applications</h3>
  <p>AI systems in this category are explicitly prohibited and cannot be developed, deployed, or used within the EU market.</p>
  
  <div class="prohibition-list">
    <div class="prohibition-item">
      <h4>ğŸ§  Subliminal Manipulation</h4>
      <p>AI systems that deploy subliminal techniques beyond a person's consciousness to materially distort behavior in a manner that causes or is likely to cause physical or psychological harm.</p>
    </div>
    
    <div class="prohibition-item">
      <h4>ğŸ¯ Exploitation of Vulnerabilities</h4>
      <p>AI systems that exploit vulnerabilities of individuals or specific groups due to their age, disability, or social or economic situation.</p>
    </div>
    
    <div class="prohibition-item">
      <h4>ğŸ“Š Social Scoring by Public Authorities</h4>
      <p>AI systems used by public authorities to evaluate or classify the trustworthiness of natural persons based on their social behavior.</p>
    </div>
    
    <div class="prohibition-item">
      <h4>ğŸ‘ï¸ Real-time Remote Biometric Identification</h4>
      <p>In publicly accessible spaces for law enforcement (with specific exceptions for serious crimes and threats).</p>
    </div>
  </div>
</div>

## âš ï¸ High-Risk AI Systems

<div class="card">
  <h3>Strict Compliance Requirements</h3>
  <p>High-risk AI systems must comply with extensive requirements including conformity assessments, CE marking, and ongoing monitoring.</p>
  
  <div class="high-risk-categories">
    <div class="category-item">
      <h4>ğŸ¥ Critical Infrastructure</h4>
      <p>AI systems used as safety components in the management and operation of critical digital infrastructure.</p>
    </div>
    
    <div class="category-item">
      <h4>ğŸ“ Education and Training</h4>
      <p>AI systems used for determining access or assigning persons to educational and vocational training institutions.</p>
    </div>
    
    <div class="category-item">
      <h4>ğŸ’¼ Employment and Workers Management</h4>
      <p>AI systems used in recruitment, promotion decisions, task allocation, and monitoring/evaluation of work performance.</p>
    </div>
    
    <div class="category-item">
      <h4>ğŸ›ï¸ Access to Essential Services</h4>
      <p>AI systems used to evaluate creditworthiness, assess eligibility for public assistance, or dispatch emergency services.</p>
    </div>
    
    <div class="category-item">
      <h4>âš–ï¸ Law Enforcement</h4>
      <p>AI systems used for risk assessment, polygraph testing, emotion recognition, or evaluating evidence reliability.</p>
    </div>
    
    <div class="category-item">
      <h4>ğŸ›‚ Migration and Border Control</h4>
      <p>AI systems used for examining applications for asylum, visa, and residence permits, or detecting document fraud.</p>
    </div>
    
    <div class="category-item">
      <h4>ğŸ›ï¸ Administration of Justice</h4>
      <p>AI systems intended to assist judicial authorities in researching and interpreting facts and law.</p>
    </div>
  </div>
</div>

## âš¡ Limited Risk AI Systems

<div class="card">
  <h3>Transparency Obligations</h3>
  <p>These AI systems must ensure that users are aware they are interacting with an AI system.</p>
  
  <div class="limited-risk-list">
    <div class="risk-item">
      <h4>ğŸ¤– AI Chatbots and Conversational Agents</h4>
      <p>Systems designed to interact directly with natural persons must clearly disclose their AI nature.</p>
    </div>
    
    <div class="risk-item">
      <h4>ğŸ­ Emotion Recognition Systems</h4>
      <p>AI systems that recognize or infer emotions, intentions, or attributes of natural persons.</p>
    </div>
    
    <div class="risk-item">
      <h4>ğŸ“º AI-Generated Content</h4>
      <p>Systems that generate or manipulate image, audio, or video content (deepfakes) must be clearly labeled.</p>
    </div>
  </div>
</div>

## âœ… Minimal Risk AI Systems

<div class="card">
  <h3>Voluntary Measures</h3>
  <p>Most AI systems fall into this category and face minimal regulatory obligations under the EU AI Act.</p>
  
  <div class="minimal-risk-examples">
    <div class="example-item">
      <h4>ğŸ® AI-Enabled Video Games</h4>
      <p>Entertainment applications with AI features.</p>
    </div>
    
    <div class="example-item">
      <h4>ğŸ” Spam Filters</h4>
      <p>AI systems for filtering unwanted content.</p>
    </div>
    
    <div class="example-item">
      <h4>ğŸ“ AI-Powered Spell Checkers</h4>
      <p>Text processing and correction tools.</p>
    </div>
    
    <div class="example-item">
      <h4>ğŸ›’ Recommendation Systems</h4>
      <p>AI systems that suggest products or content (in most contexts).</p>
    </div>
  </div>
  
  <p><strong>Note:</strong> Organizations may voluntarily adopt codes of conduct and best practices for these systems.</p>
</div>

## ğŸ¯ How to Determine Your AI System's Risk Category

<div class="card">
  <h3>Assessment Steps</h3>
  
  <div class="assessment-steps">
    <div class="step">
      <h4>Step 1: Check Prohibited List</h4>
      <p>Verify your system doesn't fall under prohibited applications.</p>
    </div>
    
    <div class="step">
      <h4>Step 2: Review High-Risk Categories</h4>
      <p>Check if your system is used in any high-risk areas or listed in Annex III.</p>
    </div>
    
    <div class="step">
      <h4>Step 3: Assess User Interaction</h4>
      <p>Determine if your system requires transparency obligations.</p>
    </div>
    
    <div class="step">
      <h4>Step 4: Default to Minimal Risk</h4>
      <p>If none of the above apply, your system likely falls under minimal risk.</p>
    </div>
  </div>
</div>

## ğŸ“š Related Resources

- [Compliance Basics](compliance-basics.html) - Learn about specific requirements for each risk category
- [Official Documents](official-docs.html) - Access official EU guidance on risk classification
- [Homepage](index.html) - Return to the main EU AI Act resource hub
